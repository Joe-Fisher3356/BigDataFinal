{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# CELL #1: Setup & Import Dependencies\n",
    "# Description: Install required packages and import libraries\n",
    "# ============================================================\n",
    "\n",
    "# !pip install selenium\n",
    "# !pip install pymongo\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install sklearn\n",
    "# !pip install matplotlib\n",
    "# !pip install BeautifulSoup\n",
    "# !pip install python-dotenv pymongo dnspython\n",
    "#!pip install wordcloud\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv"
   ],
   "id": "352b0f04a08e27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# CELL #2: Define NoFluffJobs Web Scraper Class\n",
    "# Description: Class for scraping job data from NoFluffJobs\n",
    "# Includes methods for HTML scraping, parsing, and MongoDB storage\n",
    "# ============================================================\n",
    "\n",
    "# load environment variable to init db\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# HTML webpage scrapping\n",
    "class WebScrapingNoFluff:\n",
    "    def __init__(self, query_term='backend'):\n",
    "        print('Initialise WebScraping instance')\n",
    "        self.query_term = query_term\n",
    "        self.target_url = f\"https://nofluffjobs.com/pl/?lang=en&criteria=jobPosition%3D{self.query_term}\"\n",
    "        self.final_html = ''\n",
    "        # --- init db: local db / Atlas Mongodb ---\n",
    "        self._init_db()\n",
    "\n",
    "\n",
    "    def _init_db(self):\n",
    "        \"\"\"\n",
    "        You don't need .env, cause you will deploy on local db\n",
    "        MONGO_MODE from .env:\n",
    "        \"local\": connect with local mongodb\n",
    "        \"atlas\": connect with remote mongodb\n",
    "        \"\"\"\n",
    "\n",
    "        mode = os.getenv(\"MONGO_MODE\", \"local\")\n",
    "        db_name = os.getenv(\"DB_NAME\", \"BD_final\")\n",
    "\n",
    "        print(\"DB MODE: \", mode)\n",
    "\n",
    "        if mode == \"atlas\":\n",
    "            uri = os.getenv(\"ATLAS_MONGO_URI\")\n",
    "            if not uri:\n",
    "                raise ValueError(\"‚ùå Error: ATLAS_MONGO_URI not found in .env while mode is 'atlas'\")\n",
    "            print(f\"üåê Connecting to Remote MongoDB Atlas...\")\n",
    "        else:\n",
    "            uri = \"mongodb://localhost:27017/\"\n",
    "            print(f\" Connecting to Local MongoDB ({uri})...\")\n",
    "\n",
    "        try:\n",
    "            self.client = MongoClient(uri)\n",
    "            # test connection\n",
    "            self.client.admin.command('ping')\n",
    "            self.db = self.client[db_name]\n",
    "            print(f\"‚úÖ Successfully connected to database: {db_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Database connection failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def scrape_save_raw_to_db(self, clicks=3):\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(self.target_url)\n",
    "        driver.maximize_window()\n",
    "        # Give you 15 seconds to accept/cancel all popup window\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "\n",
    "        # Loop to click \"See more offers\" button 10 times\n",
    "        count = 0\n",
    "        while count < clicks:\n",
    "            try:\n",
    "                # Find the button with the nfjloadmore attribute\n",
    "                load_more_btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[nfjloadmore]\")))\n",
    "\n",
    "                # Scroll to the button position to ensure it is in the viewport\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more_btn)\n",
    "                time.sleep(1.5)  # Allow some buffer time for scrolling\n",
    "\n",
    "                # Click to load more\n",
    "                if load_more_btn:\n",
    "                    load_more_btn.click()\n",
    "                count += 1\n",
    "                print(f\"Clicked 'See more' ({count}/{clicks})\")\n",
    "\n",
    "                # Delay for new content to load\n",
    "                time.sleep(2.5)\n",
    "            except Exception as e:\n",
    "                print(f\"Finished loading or button not found: {e}\")\n",
    "                break\n",
    "\n",
    "        # C. Get the complete HTML after 10 clicks and save it\n",
    "        print(\"All pages loaded. Capturing final HTML...\")\n",
    "        self.final_html = driver.page_source\n",
    "        self.save_raw_to_mongodb()\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "    def save_raw_to_mongodb(self):\n",
    "        \"\"\"\n",
    "        Reference your initial logic to store the raw HTML into NoSQL\n",
    "        \"\"\"\n",
    "        # only hold one raw_json html data\n",
    "        self.db.jobs_raw.drop()\n",
    "        website_document = {\n",
    "            'url': self.target_url,\n",
    "            'content': self.final_html,\n",
    "            'date': datetime.now(),\n",
    "            'query_term': self.query_term\n",
    "        }\n",
    "\n",
    "        # Store in the jobs_raw collection\n",
    "        result = self.db.jobs_raw.insert_one(website_document)\n",
    "        print(f\"Raw HTML saved to MongoDB! Document ID: {result.inserted_id}\")\n",
    "\n",
    "    # parse and split salary to min and max two value\n",
    "    def parse_salary(self, salary_str):\n",
    "        \"\"\"\n",
    "        Internal helper method: Parse salary string\n",
    "        Example input: \"10 000 - 15 000 PLN\", \"20 000 PLN\", \"Undisclosed\"\n",
    "        Output: (min_salary, max_salary)\n",
    "        \"\"\"\n",
    "        if not salary_str or \"Undisclosed\" in salary_str or \"Agreement\" in salary_str:\n",
    "            return None, None\n",
    "\n",
    "        # Remove spaces and thousands separators\n",
    "        clean_str = salary_str.replace('\\xa0', '').replace(' ', '')\n",
    "\n",
    "        # Match all digits\n",
    "        numbers = re.findall(r'\\d+', clean_str)\n",
    "\n",
    "        try:\n",
    "            if len(numbers) >= 2:\n",
    "                # Range salary: [10000, 15000]\n",
    "                return float(numbers[0]), float(numbers[1])\n",
    "            elif len(numbers) == 1:\n",
    "                # Fixed salary: [20000]\n",
    "                return float(numbers[0]), float(numbers[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return None, None\n",
    "\n",
    "    # parse job location\n",
    "    def parse_location(self, card):\n",
    "        \"\"\"\n",
    "        Internal helper method: Parse location\n",
    "        \"\"\"\n",
    "        location_tag = card.select_one('span.posting-info__location, nfj-posting-item-city')\n",
    "        if not location_tag:\n",
    "            return \"Unknown\"\n",
    "\n",
    "        loc_text = location_tag.get_text(strip=True)\n",
    "\n",
    "        # Special case handling: Remote work\n",
    "        if \"Remote\" in loc_text or \"Zdalna\" in loc_text:\n",
    "            return \"Remote\"\n",
    "\n",
    "        # Extract city name (some include '+1', filter via split)\n",
    "        city = loc_text.split('+')[0].strip()\n",
    "        return city\n",
    "\n",
    "    # parse job title,company name, Min/Max Salary,Location, Jump URL and save into db\n",
    "    def process_and_save(self):\n",
    "        \"\"\"\n",
    "        Read HTML from jobs_raw and extract fields to store in jobs_processed\n",
    "        extract raw html and parse fields then save into\n",
    "        \"\"\"\n",
    "        # Get the most recently scraped HTML document\n",
    "        raw_data = self.db.jobs_raw.find_one(sort=[(\"date\", -1)])\n",
    "        if not raw_data:\n",
    "            print(\"No raw data found in MongoDB!\")\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(raw_data['content'], 'html.parser')\n",
    "\n",
    "        # Locate all job cards\n",
    "        postings = soup.select('a.posting-list-item')\n",
    "        print(f\"Found {len(postings)} job postings in HTML.\")\n",
    "\n",
    "        processed_list = []\n",
    "        base_domain = \"https://nofluffjobs.com\"  # Used to concatenate the full URL\n",
    "\n",
    "        for post in postings:\n",
    "            try:\n",
    "                # --- Field 1: Job Title ---\n",
    "                title_el = post.select_one('h3.posting-title__position, .posting-title__can-hide')\n",
    "                if title_el:\n",
    "                    # Find and remove any potential \"NEW\" tags or other badges\n",
    "                    # NFJ's badge class names usually contain title-badge\n",
    "                    for badge in title_el.select('.title-badge, .title-badge--new'):\n",
    "                        badge.decompose()\n",
    "                    job_title = title_el.get_text(strip=True)\n",
    "                else:\n",
    "                    job_title = \"N/A\"\n",
    "                # print('JOB TITLE:', job_title)\n",
    "\n",
    "                # --- Field 2: Company Name ---\n",
    "                company_el = post.select_one('span.d-block, .company-name')\n",
    "                company_name = company_el.get_text(strip=True) if company_el else \"N/A\"\n",
    "\n",
    "                # --- Fields 3 & 4: Min/Max Salary ---\n",
    "                salary_el = post.select_one('span.text-truncate, nfj-posting-item-salary')\n",
    "                salary_str = salary_el.get_text(strip=True) if salary_el else \"\"\n",
    "                min_sal, max_sal = self.parse_salary(salary_str)\n",
    "\n",
    "                # --- Field 5: Location ---\n",
    "                location = self.parse_location(post)\n",
    "\n",
    "                # --- Field 6: Jump URL ---\n",
    "                # Get relative path from the <a> tag's href attribute and concatenate domain\n",
    "                relative_url = post.get('href')\n",
    "                jump_url = base_domain + relative_url if relative_url else \"N/A\"\n",
    "\n",
    "                # Construct document\n",
    "                job_doc = {\n",
    "                    \"source\": \"nofluffjobs\",\n",
    "                    'job_title': job_title,\n",
    "                    'company_name': company_name,\n",
    "                    'min_salary': min_sal,\n",
    "                    'max_salary': max_sal,\n",
    "                    'location': location,\n",
    "                    'jump_url': jump_url,  # Add to the document\n",
    "                    'processed_at': datetime.now(),\n",
    "                    'query_term': raw_data['query_term']\n",
    "                }\n",
    "\n",
    "                processed_list.append(job_doc)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing a single post: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Batch save to the new collection\n",
    "        if processed_list:\n",
    "            self.db.jobs_processed.drop()\n",
    "            self.db.jobs_processed.insert_many(processed_list)\n",
    "            print(f\"Successfully processed {len(processed_list)} jobs and saved to 'jobs_processed'.\")\n",
    "\n",
    "    # scrape the job detail page and add must-have skill set into each job\n",
    "    def scrape_must_have_skills(self, limit=0):\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.maximize_window()\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "\n",
    "\n",
    "        jobs = list(self.db.jobs_processed.find().limit(limit))\n",
    "        print(f\"Scraping Must-have skills for {len(jobs)} jobs\")\n",
    "\n",
    "        for job in jobs:\n",
    "            url = job.get(\"jump_url\")\n",
    "            if not url:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(2)  # wait for page to render\n",
    "\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                must_section = soup.select_one('section[branch=\"musts\"]')\n",
    "\n",
    "                skills = []\n",
    "                if must_section:\n",
    "                    skill_tags = must_section.select('span[id^=\"item-tag-\"]')\n",
    "                    for tag in skill_tags:\n",
    "                        skills.append(tag.get_text(strip=True).lower())\n",
    "\n",
    "                # Update MongoDB document\n",
    "                self.db.jobs_processed.update_one(\n",
    "                    {\"_id\": job[\"_id\"]},\n",
    "                    {\"$set\": {\"must_have_skills\": skills}}\n",
    "                )\n",
    "\n",
    "                print(f\"‚úî {job['job_title']} ‚Üí {skills}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error scraping {url} : {e}\")\n",
    "\n",
    "        driver.quit()\n",
    "        print(\"Finished scraping Must-have skills.\")"
   ],
   "id": "154958acb9b95ebf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# CELL #3: Execute NoFluffJobs Scraper\n",
    "# Description: Run the web scraper to collect raw HTML data\n",
    "# from NoFluffJobs and save it to MongoDB\n",
    "# ============================================================\n",
    "\n",
    "scraperNoFluff = WebScrapingNoFluff(query_term='backend')\n",
    "scraperNoFluff.scrape_save_raw_to_db()\n"
   ],
   "id": "57f193778b01793e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# CELL #4: Parse and Extract Structured Job Data\n",
    "# Description: Extract job title, company, salary, location\n",
    "# from raw HTML and save to MongoDB jobs_processed collection\n",
    "# ============================================================\n",
    "\n",
    "scraperNoFluff.process_and_save()"
   ],
   "id": "a4135d186db8756"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# CELL #5: Scrape Must-Have Skills\n",
    "# Description: Visit each job detail page and extract\n",
    "# the must-have skills list, adding to MongoDB documents\n",
    "# ============================================================\n",
    "\n",
    "scraperNoFluff.scrape_must_have_skills()"
   ],
   "id": "a2346e8ba351ce06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# CELL #6: Define JustJoin.it API Scraper Class\n",
    "# Description: Class for scraping job data from JustJoin.it\n",
    "# using their public API (faster than HTML scraping)\n",
    "# ============================================================\n",
    "\n",
    "class WebScrapingJustJoin:\n",
    "    def __init__(self, query_term=\"backend\"):\n",
    "        print(\"Initialise WebScrapingJustJoin instance\")\n",
    "        self.query_term = query_term\n",
    "        self.api_url = (\n",
    "            \"https://api.justjoin.it/v2/user-panel/offers/by-cursor\"\n",
    "            \"?cityRadiusKm=30\"\n",
    "            \"&currency=pln\"\n",
    "            \"&from=0\"\n",
    "            \"&itemsCount=100\"\n",
    "            f\"&keywords[]={query_term.replace(' ', '%20')}\"\n",
    "            \"&orderBy=DESC\"\n",
    "            \"&sortBy=published\"\n",
    "        )\n",
    "        self._init_db()\n",
    "\n",
    "    def _init_db(self):\n",
    "        mode = os.getenv(\"MONGO_MODE\", \"local\")\n",
    "        db_name = os.getenv(\"DB_NAME\", \"BD_final\")\n",
    "\n",
    "        if mode == \"atlas\":\n",
    "            uri = os.getenv(\"ATLAS_MONGO_URI\")\n",
    "        else:\n",
    "            uri = \"mongodb://localhost:27017/\"\n",
    "\n",
    "        self.client = MongoClient(uri)\n",
    "        self.db = self.client[db_name]\n",
    "        print(\"‚úÖ JustJoin connected to DB\")\n",
    "\n",
    "    # ---- salary normalization (matches NoFluff logic) ----\n",
    "    def normalize_salary(self, emp):\n",
    "        if not emp or emp.get(\"from\") is None:\n",
    "            return None, None\n",
    "\n",
    "        from_sal = emp.get(\"from\")\n",
    "        to_sal = emp.get(\"to\")\n",
    "        unit = emp.get(\"unit\")  # hour / day / month\n",
    "\n",
    "        if unit == \"hour\":\n",
    "            return from_sal * 160, to_sal * 160\n",
    "        if unit == \"day\":\n",
    "            return from_sal * 20, to_sal * 20\n",
    "\n",
    "        return from_sal, to_sal  # month\n",
    "\n",
    "    def scrape_and_process(self):\n",
    "        import requests\n",
    "\n",
    "        processed = []\n",
    "        items_per_request = 100   # max jobs per API request\n",
    "        max_items = 300           # total jobs we want\n",
    "\n",
    "        for start in range(0, max_items, items_per_request):\n",
    "            paged_url = (\n",
    "                \"https://api.justjoin.it/v2/user-panel/offers/by-cursor\"\n",
    "                \"?cityRadiusKm=30\"\n",
    "                \"&currency=pln\"\n",
    "                f\"&from={start}\"\n",
    "                f\"&itemsCount={items_per_request}\"\n",
    "                f\"&keywords[]={self.query_term.replace(' ', '%20')}\"\n",
    "                \"&orderBy=DESC\"\n",
    "                \"&sortBy=published\"\n",
    "            )\n",
    "\n",
    "            response = requests.get(paged_url)\n",
    "            raw = response.json()\n",
    "\n",
    "            for job in raw.get(\"data\", []):\n",
    "                emp = job.get(\"employmentTypes\", [{}])[0]\n",
    "                min_sal, max_sal = self.normalize_salary(emp)\n",
    "\n",
    "            processed.append({\n",
    "                \"source\": \"justjoin\",\n",
    "                \"job_title\": job.get(\"title\"),\n",
    "                \"company_name\": job.get(\"companyName\"),\n",
    "                \"min_salary\": min_sal,\n",
    "                \"max_salary\": max_sal,\n",
    "                \"location\": job.get(\"city\"),\n",
    "                \"jump_url\": f\"https://justjoin.it/offers/{job.get('slug')}\",\n",
    "                \"must_have_skills\": [\n",
    "                    skill.lower() for skill in job.get(\"requiredSkills\", [])\n",
    "                ],\n",
    "                \"processed_at\": datetime.now(),\n",
    "                \"query_term\": self.query_term\n",
    "            })\n",
    "\n",
    "        self.db.jobs_processed_jj.drop()\n",
    "        self.db.jobs_processed_jj.insert_many(processed)\n",
    "        print(f\"‚úÖ Saved {len(processed)} JustJoin jobs\")\n"
   ],
   "id": "dfa067853adfe7cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# CELL #7: Execute JustJoin.it API Scraper\n",
    "# Description: Scrape jobs from JustJoin.it and save to\n",
    "# MongoDB jobs_processed_jj collection (includes skills)\n",
    "# ============================================================\n",
    "\n",
    "jj_scraper = WebScrapingJustJoin(query_term=\"backend\")\n",
    "jj_scraper.scrape_and_process()"
   ],
   "id": "829d70d3ceb824e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# CELL #8: Load Jobs from Both Sources into DataFrame\n",
    "# Description: Retrieves job data from both NoFluffJobs and\n",
    "# JustJoin.it collections, combines them into a pandas DataFrame\n",
    "# with standardized skills formatting\n",
    "# ============================================================\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"BD_final\"]\n",
    "\n",
    "# combine both sources into one dataset\n",
    "jobs_nf = list(db.jobs_processed.find({\"must_have_skills\": {\"$exists\": True, \"$ne\": []}}))\n",
    "jobs_jj = list(db.jobs_processed_jj.find({\"must_have_skills\": {\"$exists\": True, \"$ne\": []}}))\n",
    "\n",
    "jobs = jobs_nf + jobs_jj\n",
    "\n",
    "# Preserve all relevant metadata, including jump_url and query_term\n",
    "df = pd.DataFrame(jobs)[[\n",
    "    \"job_title\",\n",
    "    \"company_name\",\n",
    "    \"must_have_skills\",\n",
    "    \"min_salary\",\n",
    "    \"max_salary\",\n",
    "    \"source\"\n",
    "]]\n",
    "\n",
    "# Drop rows with empty or missing skills\n",
    "df = df[df[\"must_have_skills\"].apply(lambda x: isinstance(x, list) and len(x) > 0)].copy()\n",
    "\n",
    "# Lowercase all skills and join into a single string per job\n",
    "df[\"skills_text\"] = df[\"must_have_skills\"].apply(lambda x: \" \".join([s.lower() for s in x]))\n",
    "\n",
    "print(f\"Loaded {len(df)} jobs with skills\")"
   ],
   "id": "466aa7d6eeefbd03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# CELL #9: Skill Frequency Analysis with MongoDB Aggregation\n",
    "# Description: Performs advanced aggregation pipeline to analyze\n",
    "# skill frequency, salary ranges, and job title distribution\n",
    "# across both job sources\n",
    "# ============================================================\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Connect to MongoDB (adjust the connection string as needed)\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['BD_final']\n",
    "\n",
    "# Define the pipeline without the unionWith operation\n",
    "pipeline = [\n",
    "    # 1. Remove jobs without skills\n",
    "    {\n",
    "        \"$match\": {\n",
    "            \"must_have_skills\": {\"$exists\": True, \"$ne\": []}\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # 2. Explode skills array ‚Üí one document per skill\n",
    "    {\n",
    "        \"$unwind\": \"$must_have_skills\"\n",
    "    },\n",
    "\n",
    "    # 3. Group by skill\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$must_have_skills\",\n",
    "\n",
    "            # how many job postings mention this skill\n",
    "            \"job_count\": {\"$sum\": 1},\n",
    "\n",
    "            # salary context (Mongo ignores nulls automatically)\n",
    "            \"avg_min_salary\": {\"$avg\": \"$min_salary\"},\n",
    "            \"avg_max_salary\": {\"$avg\": \"$max_salary\"},\n",
    "\n",
    "            # keep example values (useful for inspection)\n",
    "            \"example_titles\": {\"$addToSet\": \"$job_title\"}\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # 4. Add derived fields\n",
    "    {\n",
    "        \"$addFields\": {\n",
    "            \"unique_titles_count\": {\"$size\": \"$example_titles\"}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run pipeline on each collection separately\n",
    "skill_analysis_1 = list(db.jobs_processed.aggregate(pipeline))\n",
    "skill_analysis_2 = list(db.jobs_processed_jj.aggregate(pipeline))\n",
    "\n",
    "# Combine results properly by merging skills\n",
    "combined_skills = defaultdict(lambda: {\n",
    "    \"job_count\": 0,\n",
    "    \"salary_sum_min\": 0,\n",
    "    \"salary_sum_max\": 0,\n",
    "    \"salary_count\": 0,\n",
    "    \"example_titles\": set()\n",
    "})\n",
    "\n",
    "# Process first collection results\n",
    "for skill in skill_analysis_1:\n",
    "    skill_name = skill[\"_id\"]\n",
    "    combined_skills[skill_name][\"job_count\"] += skill[\"job_count\"]\n",
    "\n",
    "    # Handle salary data\n",
    "    if \"avg_min_salary\" in skill and skill[\"avg_min_salary\"] is not None:\n",
    "        combined_skills[skill_name][\"salary_sum_min\"] += skill[\"avg_min_salary\"] * skill[\"job_count\"]\n",
    "        combined_skills[skill_name][\"salary_count\"] += skill[\"job_count\"]\n",
    "\n",
    "    if \"avg_max_salary\" in skill and skill[\"avg_max_salary\"] is not None:\n",
    "        combined_skills[skill_name][\"salary_sum_max\"] += skill[\"avg_max_salary\"] * skill[\"job_count\"]\n",
    "\n",
    "    # Add example titles\n",
    "    combined_skills[skill_name][\"example_titles\"].update(skill[\"example_titles\"])\n",
    "\n",
    "# Process second collection results\n",
    "for skill in skill_analysis_2:\n",
    "    skill_name = skill[\"_id\"]\n",
    "    combined_skills[skill_name][\"job_count\"] += skill[\"job_count\"]\n",
    "\n",
    "    # Handle salary data\n",
    "    if \"avg_min_salary\" in skill and skill[\"avg_min_salary\"] is not None:\n",
    "        combined_skills[skill_name][\"salary_sum_min\"] += skill[\"avg_min_salary\"] * skill[\"job_count\"]\n",
    "        combined_skills[skill_name][\"salary_count\"] += skill[\"job_count\"]\n",
    "\n",
    "    if \"avg_max_salary\" in skill and skill[\"avg_max_salary\"] is not None:\n",
    "        combined_skills[skill_name][\"salary_sum_max\"] += skill[\"avg_max_salary\"] * skill[\"job_count\"]\n",
    "\n",
    "    # Add example titles\n",
    "    combined_skills[skill_name][\"example_titles\"].update(skill[\"example_titles\"])\n",
    "\n",
    "# Convert to final format\n",
    "skill_analysis = []\n",
    "for skill_name, data in combined_skills.items():\n",
    "    skill_doc = {\n",
    "        \"_id\": skill_name,\n",
    "        \"job_count\": data[\"job_count\"],\n",
    "        \"example_titles\": list(data[\"example_titles\"]),\n",
    "        \"unique_titles_count\": len(data[\"example_titles\"])\n",
    "    }\n",
    "\n",
    "    # Calculate average salaries if data exists\n",
    "    if data[\"salary_count\"] > 0:\n",
    "        skill_doc[\"avg_min_salary\"] = data[\"salary_sum_min\"] / data[\"salary_count\"]\n",
    "        skill_doc[\"avg_max_salary\"] = data[\"salary_sum_max\"] / data[\"salary_count\"]\n",
    "\n",
    "    skill_analysis.append(skill_doc)\n",
    "\n",
    "# Sort by job count (most in-demand skills first)\n",
    "skill_analysis.sort(key=lambda x: x[\"job_count\"], reverse=True)\n",
    "\n",
    "# Quick inspection\n",
    "for doc in skill_analysis[:10]:\n",
    "    print(doc)"
   ],
   "id": "e46db1f3a55804a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# CELL #10: TF-IDF Vectorization of Skills\n",
    "# Description: Transforms job skills into a numerical matrix using\n",
    "# TF-IDF vectorization for subsequent analysis and modeling\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=0.015,   # ignore very rare skills\n",
    "    max_df=0.99\n",
    ")\n",
    "\n",
    "X_skills = vectorizer.fit_transform(df[\"skills_text\"])\n",
    "print(\"Skill matrix shape:\", X_skills.shape)"
   ],
   "id": "ea192a8c05428dca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# CELL #11: Determine Optimal Number of Clusters\n",
    "# Description: Uses elbow method and silhouette scores to identify\n",
    "# the optimal number of skill clusters for backend job market\n",
    "# ============================================================\n",
    "\n",
    "# Estimate reasonable number of clusters for backend job skills\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# Count unique skill combinations\n",
    "df[\"skills_tuple\"] = df[\"must_have_skills\"].apply(lambda x: tuple(sorted(x)))\n",
    "skill_combos = df.groupby(\"skills_tuple\").size().sort_values(ascending=False)\n",
    "\n",
    "print(f\"Total jobs: {len(df)}\")\n",
    "print(f\"Total unique skill combinations: {len(skill_combos)}\")\n",
    "print(\"Top 10 most common skill combinations:\")\n",
    "print(skill_combos.head(10))\n",
    "\n",
    "# Heuristic for number of clusters\n",
    "heuristic_k = int(np.sqrt(len(df)/2))\n",
    "print(f\"\\nHeuristic suggestion for max number of clusters: {heuristic_k}\")\n",
    "\n",
    "# Silhouette scores for k=2 to 9\n",
    "print(\"\\nSilhouette scores for different k:\")\n",
    "for k in range(2, 20):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
    "    labels = kmeans.fit_predict(X_skills)\n",
    "    score = silhouette_score(X_skills, labels)\n",
    "    print(f\"k={k}, silhouette score={score:.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Use this information to pick an interpretable k\")"
   ],
   "id": "b431a8e6baf8ba6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# CELL #12: Determine Optimal Number of Clusters\n",
    "# Description: Uses elbow method and silhouette scores to identify\n",
    "# the optimal number of skill clusters for backend job market\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Standardize X_skills (optional, can use as-is since TF-IDF is normalized)\n",
    "X_dense = X_skills.toarray()\n",
    "\n",
    "K_range = range(2, 20)\n",
    "distortions = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
    "    labels = kmeans.fit_predict(X_dense)\n",
    "\n",
    "    # distortion: mean distance to closest centroid\n",
    "    dist = np.mean(np.min(cdist(X_dense, kmeans.cluster_centers_, 'euclidean'), axis=1))\n",
    "    distortions.append(dist)\n",
    "\n",
    "    sil = silhouette_score(X_dense, labels)\n",
    "    silhouette_scores.append(sil)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(K_range, distortions, 'bx-', linewidth=2)\n",
    "plt.xlabel(\"Number of clusters k\")\n",
    "plt.ylabel(\"Distortion\")\n",
    "plt.title(\"Elbow Method (Distortion)\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(K_range, silhouette_scores, 'ro-', linewidth=2)\n",
    "plt.xlabel(\"Number of clusters k\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Score per k\")\n",
    "\n",
    "plt.show()"
   ],
   "id": "d70c9bc550fe531e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# CELL #13: Apply K-Means Clustering and Analyze Skill Groups\n",
    "# Description: Implements final clustering with optimal k=12,\n",
    "# identifies key skills in each cluster, and provides examples\n",
    "# of representative job titles for each skill group\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Use the optimal k=12 based on silhouette analysis\n",
    "k_optimal = 12\n",
    "\n",
    "# Apply KMeans with the optimal k\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = kmeans.fit_predict(X_dense)\n",
    "\n",
    "# Get cluster sizes\n",
    "cluster_sizes = df[\"cluster\"].value_counts().sort_index()\n",
    "print(f\"Jobs per cluster:\")\n",
    "for i in range(k_optimal):\n",
    "    print(f\"Cluster {i}: {cluster_sizes.get(i, 0)} jobs\")\n",
    "\n",
    "# Extract feature names from the vectorizer\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display top skills for each cluster\n",
    "print(\"\\nTop skills per cluster:\")\n",
    "for cluster_id in range(k_optimal):\n",
    "    center = kmeans.cluster_centers_[cluster_id]\n",
    "    top_idx = np.argsort(center)[-10:][::-1]\n",
    "    top_skills = [terms[i] for i in top_idx]\n",
    "\n",
    "    # Get the weights for the top skills\n",
    "    top_weights = center[top_idx]\n",
    "\n",
    "    print(f\"\\nCluster {cluster_id} ({cluster_sizes.get(cluster_id, 0)} jobs):\")\n",
    "    # Print skills with their weights\n",
    "    for skill, weight in zip(top_skills, top_weights):\n",
    "        print(f\"  - {skill}: {weight:.3f}\")\n",
    "\n",
    "    # Show example job titles from this cluster\n",
    "    if cluster_id in cluster_sizes.index:\n",
    "        examples = df[df[\"cluster\"] == cluster_id][\"job_title\"].sample(min(3, cluster_sizes[cluster_id])).tolist()\n",
    "        print(f\"  Example jobs: {examples}\")\n",
    "\n",
    "print(\"\\n‚úÖ Skill clusters identified with their most representative technologies\")"
   ],
   "id": "3b15f56e669ec737"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# CELL #14: Salary Analysis by Cluster\n",
    "# Description: Analyzes salary distributions across skill clusters\n",
    "# to identify high-value skill combinations in the job market\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Filter to jobs with salary data\n",
    "salary_df = df[df[\"min_salary\"].notna() & df[\"max_salary\"].notna()].copy()\n",
    "\n",
    "# Calculate average salary for each job\n",
    "salary_df[\"avg_salary\"] = (salary_df[\"min_salary\"] + salary_df[\"max_salary\"]) / 2\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR  # Standard outlier threshold\n",
    "\n",
    "    return df[df[column] <= upper_bound]\n",
    "\n",
    "# Apply outlier removal\n",
    "salary_df_filtered = remove_outliers(salary_df, \"avg_salary\")\n",
    "print(f\"Removed {len(salary_df) - len(salary_df_filtered)} outliers from {len(salary_df)} jobs with salary data\")\n",
    "\n",
    "# 1. Summary statistics by cluster using filtered data\n",
    "cluster_salary_stats = salary_df_filtered.groupby(\"cluster\")[\"avg_salary\"].agg(\n",
    "    [\"count\", \"mean\", \"median\", \"std\", \"min\", \"max\"]\n",
    ").sort_values(by=\"median\", ascending=False)\n",
    "\n",
    "print(\"Salary Statistics by Cluster (sorted by median salary):\")\n",
    "print(cluster_salary_stats)\n",
    "\n",
    "# 2. Visualize salary distributions with filtered data\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x=\"cluster\", y=\"avg_salary\", data=salary_df_filtered, order=cluster_salary_stats.index)\n",
    "plt.title(\"Salary Distribution by Skill Cluster (Outliers Removed)\", fontsize=16)\n",
    "plt.xlabel(\"Cluster\", fontsize=14)\n",
    "plt.ylabel(\"Average Salary (PLN)\", fontsize=14)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add cluster labels below the x-axis\n",
    "cluster_labels = []\n",
    "for cluster_id in cluster_salary_stats.index:\n",
    "    # Get top 3 skills for this cluster\n",
    "    center = kmeans.cluster_centers_[cluster_id]\n",
    "    top_idx = np.argsort(center)[-3:][::-1]\n",
    "    top_skills = [terms[i] for i in top_idx]\n",
    "    cluster_labels.append(f\"{cluster_id}: {', '.join(top_skills)}\")\n",
    "\n",
    "plt.xticks(range(len(cluster_labels)), cluster_labels, rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Identify highest and lowest paying clusters\n",
    "highest_paying = cluster_salary_stats.index[0]\n",
    "lowest_paying = cluster_salary_stats.index[-1]\n",
    "\n",
    "print(f\"\\nHighest paying cluster ({highest_paying}):\")\n",
    "top_skills_highest = [terms[i] for i in np.argsort(kmeans.cluster_centers_[highest_paying])[-10:][::-1]]\n",
    "print(f\"Top skills: {', '.join(top_skills_highest)}\")\n",
    "print(f\"Median salary: {cluster_salary_stats.loc[highest_paying, 'median']:.2f} PLN\")\n",
    "\n",
    "print(f\"\\nLowest paying cluster ({lowest_paying}):\")\n",
    "top_skills_lowest = [terms[i] for i in np.argsort(kmeans.cluster_centers_[lowest_paying])[-10:][::-1]]\n",
    "print(f\"Top skills: {', '.join(top_skills_lowest)}\")\n",
    "print(f\"Median salary: {cluster_salary_stats.loc[lowest_paying, 'median']:.2f} PLN\")\n",
    "\n",
    "print(\"\\n‚úÖ Salary analysis complete - identified high-value skill combinations\")"
   ],
   "id": "3404757ab20e9f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================\n",
    "# CELL #15: Skill Gap and Demand Analysis\n",
    "# Description: Identifies valuable skills that appear predominantly\n",
    "# in higher-paying clusters and analyzes demand vs. compensation\n",
    "# ============================================================\n",
    "\n",
    "# Calculate skill importance across clusters\n",
    "skill_importance = {}\n",
    "\n",
    "# Extract feature names from the vectorizer\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# For each cluster, calculate importance of each skill\n",
    "for cluster_id in range(k_optimal):\n",
    "    # Get the cluster center\n",
    "    center = kmeans.cluster_centers_[cluster_id]\n",
    "\n",
    "    # If this is the first cluster, initialize the dictionary with all skills\n",
    "    if cluster_id == 0:\n",
    "        for i, skill in enumerate(terms):\n",
    "            skill_importance[skill] = [0] * k_optimal\n",
    "\n",
    "    # Store the importance (weight) of each skill in this cluster\n",
    "    for i, skill in enumerate(terms):\n",
    "        skill_importance[skill][cluster_id] = center[i]\n",
    "\n",
    "# Now skill_importance is a dictionary where:\n",
    "# - Keys are skill names\n",
    "# - Values are lists of importance scores for each cluster\n",
    "\n",
    "print(f\"Skill importance calculated for {len(skill_importance)} skills across {k_optimal} clusters\")\n",
    "\n",
    "# Optional: You can also calculate overall importance by summing or averaging across clusters\n",
    "overall_importance = {skill: sum(scores) for skill, scores in skill_importance.items()}\n",
    "\n",
    "# Example: Show top 10 skills by overall importance\n",
    "top_skills = sorted(overall_importance.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"\\nTop 10 skills by overall importance:\")\n",
    "for skill, score in top_skills:\n",
    "    print(f\"  - {skill}: {score:.3f}\")\n",
    "\n",
    "# Use the actual number of clusters from your data\n",
    "k_optimal = len(skill_importance[list(skill_importance.keys())[0]])\n",
    "print(f\"Using k_optimal = {k_optimal}\")\n",
    "\n",
    "# 1. Calculate skill importance across all clusters\n",
    "for term_idx, term in enumerate(terms):\n",
    "    # Get the weight of this skill in each cluster\n",
    "    weights = [center[term_idx] for center in kmeans.cluster_centers_]\n",
    "    skill_importance[term] = weights\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "skill_df = pd.DataFrame(skill_importance).T\n",
    "skill_df.columns = [f\"cluster_{i}\" for i in range(k_optimal)]\n",
    "\n",
    "# 2. Calculate correlation between skill presence and cluster salary\n",
    "cluster_median_salaries = cluster_salary_stats[\"median\"].to_dict()\n",
    "salary_correlation = {}\n",
    "\n",
    "for skill in skill_df.index:\n",
    "    # Weight each cluster's importance by its median salary\n",
    "    weighted_importance = sum(\n",
    "        skill_df.loc[skill, f\"cluster_{c}\"] * cluster_median_salaries.get(c, 0)\n",
    "        for c in range(k_optimal)\n",
    "    )\n",
    "    salary_correlation[skill] = weighted_importance\n",
    "\n",
    "# Sort skills by salary correlation\n",
    "salary_corr_df = pd.DataFrame({\n",
    "    \"skill\": list(salary_correlation.keys()),\n",
    "    \"salary_correlation\": list(salary_correlation.values())\n",
    "}).sort_values(\"salary_correlation\", ascending=False)\n",
    "\n",
    "# 3. Calculate skill demand (frequency across all jobs)\n",
    "skill_demand = {}\n",
    "# Use the same set of skills as in salary_correlation to ensure consistency\n",
    "for skill in salary_correlation.keys():\n",
    "    # Count jobs that require this skill\n",
    "    demand = df[\"skills_text\"].str.contains(r'\\b' + skill + r'\\b').sum()\n",
    "    skill_demand[skill] = demand / len(df)  # Normalize by total jobs\n",
    "\n",
    "# 4. Combine salary correlation and demand\n",
    "skill_analysis = salary_corr_df.copy()\n",
    "skill_analysis[\"demand\"] = skill_analysis[\"skill\"].map(skill_demand)\n",
    "\n",
    "# Filter to skills with meaningful presence (at least 5% of jobs)\n",
    "skill_analysis = skill_analysis[skill_analysis[\"demand\"] >= 0.05].reset_index(drop=True)\n",
    "\n",
    "# Define the t_values function to sort the skill_analysis DataFrame\n",
    "def t_values(column_name, ascending=True):\n",
    "    return skill_analysis.sort_values(by=column_name, ascending=ascending)\n",
    "\n",
    "# 5. Identify high-value skills (high salary correlation, lower demand)\n",
    "print(\"Top 20 High-Value Skills (High Salary Correlation):\")\n",
    "print(skill_analysis.head(20))\n",
    "\n",
    "# 6. Visualize skill value vs. demand with smart label placement for top 20 high-value skills\n",
    "import matplotlib.patheffects as PathEffects\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Create a scatter plot with color gradient based on salary correlation for all points\n",
    "scatter = plt.scatter(\n",
    "    skill_analysis[\"demand\"],\n",
    "    skill_analysis[\"salary_correlation\"],\n",
    "    alpha=0.8,\n",
    "    c=skill_analysis[\"salary_correlation\"],  # Color by salary correlation\n",
    "    cmap='viridis',                          # Use a color gradient\n",
    "    s=80                                     # Slightly larger points\n",
    ")\n",
    "\n",
    "# Add a color bar to show the salary correlation scale\n",
    "plt.colorbar(scatter, label=\"Salary Correlation\")\n",
    "\n",
    "# Get the top 20 skills by salary correlation\n",
    "top20_skills = t_values(\"salary_correlation\", ascending=False).head(20)\n",
    "\n",
    "# Highlight the top 20 skills with a distinct border\n",
    "for i, row in top20_skills.iterrows():\n",
    "    plt.scatter(\n",
    "        row[\"demand\"],\n",
    "        row[\"salary_correlation\"],\n",
    "        s=150,              # Larger size\n",
    "        facecolors='none',  # Transparent fill\n",
    "        edgecolors='white', # White border\n",
    "        linewidths=2,       # Thicker border\n",
    "        zorder=10           # Ensure it's on top\n",
    "    )\n",
    "\n",
    "# Add labels with varying offsets to reduce overlap\n",
    "offsets = [(7, 0), (7, 10), (7, -10), (-40, 0), (-40, 10), (-40, -10),\n",
    "           (7, 20), (7, -20), (-40, 20), (-40, -20)]\n",
    "\n",
    "for i, row in top20_skills.iterrows():\n",
    "    # Use different offsets for different points\n",
    "    offset = offsets[i % len(offsets)]\n",
    "\n",
    "    # Draw a line from the data point to the label if offset is significant\n",
    "    if abs(offset[0]) > 10 or abs(offset[1]) > 5:\n",
    "        plt.annotate(\n",
    "            '',\n",
    "            xy=(row[\"demand\"], row[\"salary_correlation\"]),\n",
    "            xytext=(row[\"demand\"] + offset[0]/500, row[\"salary_correlation\"] + offset[1]/500),\n",
    "            arrowprops=dict(arrowstyle='-', color='white', lw=1, alpha=0.7),\n",
    "            zorder=9\n",
    "        )\n",
    "\n",
    "    # Add the label\n",
    "    plt.annotate(\n",
    "        row[\"skill\"],\n",
    "        xy=(row[\"demand\"], row[\"salary_correlation\"]),\n",
    "        xytext=offset,\n",
    "        textcoords=\"offset points\",\n",
    "        fontsize=11,\n",
    "        fontweight='bold',\n",
    "        color='white',\n",
    "        path_effects=[\n",
    "            PathEffects.withStroke(linewidth=3, foreground='black')\n",
    "        ],\n",
    "        va='center',\n",
    "        zorder=11\n",
    "    )\n",
    "\n",
    "plt.title(\"Top 20 High-Value Skills by Salary Correlation\", fontsize=16)\n",
    "plt.xlabel(\"Demand (% of Jobs Requiring Skill)\", fontsize=14)\n",
    "plt.ylabel(\"Salary Correlation\", fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Just print the completion message without the redundant list\n",
    "print(\"\\n‚úÖ Skill gap analysis complete - identified high-value skills with varying demand\")"
   ],
   "id": "967bcce686b5e48b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Train model and visualization except must_have_skills\n",
    "# -------------------------\n",
    "# 1Ô∏è‚É£ Import libraries\n",
    "# -------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Run this function to make sure we can query data from Mongodb Atlas and local DB\n",
    "def init_db():\n",
    "    mode = os.getenv(\"MONGO_MODE\", \"local\")\n",
    "    db_name = os.getenv(\"DB_NAME\", \"BD_final\")\n",
    "\n",
    "    print(\"DB MODE: \", mode)\n",
    "\n",
    "    if mode == \"atlas\":\n",
    "        uri = os.getenv(\"ATLAS_MONGO_URI\")\n",
    "        if not uri:\n",
    "            raise ValueError(\"‚ùå Error: ATLAS_MONGO_URI not found in .env while mode is 'atlas'\")\n",
    "        print(f\"üåê Connecting to Remote MongoDB Atlas...\")\n",
    "    else:\n",
    "        uri = \"mongodb://localhost:27017/\"\n",
    "        print(f\" Connecting to Local MongoDB ({uri})...\")\n",
    "\n",
    "    try:\n",
    "        my_client = MongoClient(uri)\n",
    "        # test connection\n",
    "        my_client.admin.command('ping')\n",
    "        my_db = my_client[db_name]\n",
    "        print(f\"‚úÖ Successfully connected to database: {db_name}\")\n",
    "        return my_db\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Database connection failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Initialized DB\n",
    "db = init_db()\n",
    "\n",
    "# function train_and_visualise(platform_name), platform_name = JUST_JOIN / NO_FLUFF_JOBS\n",
    "from enum import Enum\n",
    "\n",
    "class CollectionEnum(Enum):\n",
    "    JUST_JOIN = \"JustJoin\"\n",
    "    NO_FLUFF_JOBS = \"NoFluffJobs\"\n",
    "\n",
    "\n",
    "def job_level(title):\n",
    "    title = title.lower()\n",
    "    if \"senior\" in title:\n",
    "        return \"senior\"\n",
    "    if \"junior\" in title:\n",
    "        return \"junior\"\n",
    "    if \"mid\" in title or \"regular\" in title:\n",
    "        return \"mid\"\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "def train_and_visualise(platform_name):\n",
    "    if platform_name == CollectionEnum.JUST_JOIN:\n",
    "        data = list(db.jobs_processed_jj.find())\n",
    "    elif platform_name == CollectionEnum.NO_FLUFF_JOBS:\n",
    "        data = list(db.jobs_processed.find())\n",
    "    else:\n",
    "        raise NameError(\"Can not find the correct collection name\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 2Ô∏è‚É£ Load data from MongoDB\n",
    "    # -------------------------\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Keep only relevant columns that exist\n",
    "    df = df[[\n",
    "        \"source\",\n",
    "        \"job_title\",\n",
    "        \"company_name\",\n",
    "        \"min_salary\",\n",
    "        \"max_salary\",\n",
    "        \"location\"\n",
    "    ]]\n",
    "\n",
    "    # Drop rows without salary\n",
    "    df = df.dropna(subset=[\"min_salary\", \"max_salary\"])\n",
    "\n",
    "    # Target variable: average salary\n",
    "    df[\"avg_salary\"] = (df[\"min_salary\"] + df[\"max_salary\"]) / 2\n",
    "\n",
    "    # -------------------------\n",
    "    # 3Ô∏è‚É£ Feature engineering\n",
    "    # -------------------------\n",
    "    # Extract job level from job title\n",
    "\n",
    "    df[\"job_level\"] = df[\"job_title\"].apply(job_level)\n",
    "\n",
    "    # Features and target\n",
    "    X = df[[\"location\", \"source\", \"job_level\"]]\n",
    "    y = df[\"avg_salary\"]\n",
    "\n",
    "    # -------------------------\n",
    "    # 4Ô∏è‚É£ Encode categorical + train model\n",
    "    # -------------------------\n",
    "    categorical_features = [\"location\", \"source\", \"job_level\"]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"regressor\", RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # -------------------------\n",
    "    # 5Ô∏è‚É£ Evaluate model\n",
    "    # -------------------------\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "    print(f\"R¬≤ Score: {r2:.2f}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 6Ô∏è‚É£ Visualization\n",
    "    # -------------------------\n",
    "\n",
    "    # Salary distribution\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(df[\"avg_salary\"], bins=30, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel(\"Average Monthly Salary (PLN)\")\n",
    "    plt.ylabel(\"Number of Jobs\")\n",
    "    plt.title(platform_name.value + \" Salary Distribution \")\n",
    "    plt.show()\n",
    "\n",
    "    # Salary by job level\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    df.boxplot(column=\"avg_salary\", by=\"job_level\")\n",
    "    plt.xlabel(\"Job Level\")\n",
    "    plt.ylabel(\"Average Salary (PLN)\")\n",
    "    plt.title(\"Salary by Job Level\")\n",
    "    plt.suptitle(\"\")  # remove default title\n",
    "    plt.show()\n",
    "\n",
    "    # Salary by source platform\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    df.boxplot(column=\"avg_salary\", by=\"source\")\n",
    "    plt.xlabel(\"Source Platform\")\n",
    "    plt.ylabel(\"Average Salary (PLN)\")\n",
    "    plt.title(\"Salary by Source Platform\")\n",
    "    plt.suptitle(\"\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "train_and_visualise(CollectionEnum.JUST_JOIN)\n",
    "train_and_visualise(CollectionEnum.NO_FLUFF_JOBS)\n"
   ],
   "id": "bf64207ee33c513d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "def visualize_by_word_cloud(platform_name, ax, number_of_words_visible=20):\n",
    "    \"\"\"\n",
    "    Fetches job data from MongoDB, processes 'must_have_skills',\n",
    "    and generates a word cloud on the provided axis.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    if platform_name == CollectionEnum.JUST_JOIN:\n",
    "        data = list(db.jobs_processed_jj.find())\n",
    "    elif platform_name == CollectionEnum.NO_FLUFF_JOBS:\n",
    "        data = list(db.jobs_processed.find())\n",
    "    else:\n",
    "        raise NameError(\"Cannot find the correct collection name\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Identify skill columns containing the keyword 'must_have_skills'\n",
    "    skill_cols = [col for col in df.columns if 'must_have_skills' in col]\n",
    "    raw_skills = df[skill_cols].values.flatten()\n",
    "\n",
    "\n",
    "    all_skills = []\n",
    "    for item in raw_skills:\n",
    "\n",
    "        if isinstance(item, (list, np.ndarray)):\n",
    "            for s in item:\n",
    "                if pd.notna(s) and str(s).strip() != '':\n",
    "                    all_skills.append(str(s).strip().lower())\n",
    "        # Handle single string values\n",
    "        elif pd.notna(item) and str(item).strip() != '':\n",
    "            all_skills.append(str(item).strip().lower())\n",
    "\n",
    "\n",
    "    skill_counts = Counter(all_skills)\n",
    "\n",
    "\n",
    "    wc = WordCloud(\n",
    "        width=600,\n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        colormap='viridis',\n",
    "        max_words=number_of_words_visible\n",
    "    ).generate_from_frequencies(skill_counts)\n",
    "\n",
    "\n",
    "    ax.imshow(wc, interpolation='bilinear')\n",
    "    ax.set_title(f\"Platform: {platform_name.value}\", fontsize=18, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "# Create a figure with 1 row and 2 columns for side-by-side comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "\n",
    "visualize_by_word_cloud(CollectionEnum.NO_FLUFF_JOBS, ax=ax1)\n",
    "visualize_by_word_cloud(CollectionEnum.JUST_JOIN, ax=ax2)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.savefig('combined_skills_comparison.png')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "bf7b39c312393bfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n",
   "id": "7a649f0cebfe06c8"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
